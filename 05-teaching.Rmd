# Teaching Examples {#teaching}

This chapter is going to have a bunch of teaching examples in it. 





## The Anscombe Exercise {#anscombe}

##About This Exercise
**Exercise Type:** Confidence Builder

*Confidence Builders are designed to get you more comfortable with R. We'll go through bits of code more or less step-by-step, and describe the reasoning behind certain actions. Remember - it's not cheating to copy code from this document and paste it into your R Studio console!*

**Exercise Suitability:** Beginner

*If you have R Studio installed on your machine and you know what an object is, you can work through this material.*

## The Anscombe Exercise
This exercise will take you through a really fun little dataset called the *Anscombe Quartet*. I've been trying to teach this exercise using data I've made up myself, and it's never quite worked. Imagine my delight when I heard that Francis Anscombe had already created the perfect dataset for me! At least, according to Wikipedia, no one knows exactly how he came up with it.

The **aim** of this exercise is to make you more comfortable with entering code in R. 

## Learning Outcomes
By the end of this workshop you should be able to . . .

* Provide descriptive statistics for data
* Run a simple regression on data
* Change arguments in a function
* Create visualisations in R


## The R Environment
Here are the packages you'll need for this exercise. (Remember, if you don't have one of these packages you can install it with the command: `install.packages("packagename")`
```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(gridExtra)
library(pastecs)
library(tidyr)
library(dplyr)
```
```{r, echo = FALSE}
opts = options(digits = 3)
```


## The Anscombe Dataset
The Anscombe dataset is really interesting and comes pre-loaded as part of the package `datasets`. This is a package which comes as standard in any R installation and so we can look at it immediately just by typing the name of the dataset into R Studio. 

R Studio will show us all rows of this dataset because it's quite small (only 11 rows). If we had a larger dataset, it would cut off after about 10 rows.

```{r}
anscombe 
```



## Running Descriptive Statistics
Let's start by exploring the means and medians of all these variables. There is a very handy command `stat.desc` from the package`pastecs` which can do this for us. 

(Note - there are LOADS of packages which give you descriptive stats. Check out [this statmethods page](https://www.statmethods.net/stats/descriptives.html) for more examples. I just personally like `pastecs`.)

```{r}
stat.desc(anscombe)
```

For each variable in the array, `stat.desc` gives us:

* `nbr.val` Number of values within the variable
* `nbr.null` Number of null values
* `nbr.na` Number of missing values
* `min` Minimum value
* `max` Maximum value
* `range` Maximum-minimum value
* `sum` Sum of all non-missing values
* `median` The median value
* `mean` The mean value
* `SE.mean` The standard error of the mean
* `CI.mean.0.95` The 95% confidence intervals of this mean
* `var` The variance of this variable
* `std.dev` The standard deviation of the mean
* `coef.var` The variation coefficient (standard deviation/mean)

With these descriptive statistics, we might want to start writing our interpretation of the data. For example ...

> The average score of *x* was 9 Â±3.3 standard deviations.
### Changing Arguments in a Function

You might decide you don't want to bother with *all* the elements of `stat.desc` and so you could explore how to modify the command by asking R `help("stat.desc")` which will show you the help documentation for the function. Note that it talks about *arguments*. 

```{r}
stat.desc(anscombe, basic=FALSE, norm=FALSE, p=0.99)
```

By setting the arguments `basic=FALSE` and `norm=FALSE`, we've told R we only care about the information that the `desc` argument gives us. Note, because we only want the `desc` argument we don't need to bother typing it in. It's set to `true` by default. You'd get the same result by typing `stat.desc(anscombe, basic=FALSE, desc=TRUE, norm=FALSE, p=0.99)` which you can try yourself. We've also changed the confidence interval width to 0.99 (i.e. 99 %). 

In this example, there's no real harm in getting all the information `stat.desc` can give us, but when we start plotting the data we will want to play about with the arguments in the `ggplot` command, so that's why you're seeing them here!

## Run a Regression
We're going to run four regressions for this dataset (all four *x*s against all 4 *y*s) and explore them. This will be done using R's inbuilt stats package, so there's no need to load any specific package into our library. 
```{r}
regression1 <- lm (y1~x1, data=anscombe)
regression2 <- lm (y2~x2, data=anscombe)
regression3 <- lm (y3~x3, data=anscombe)
regression4 <- lm (y4~x4, data=anscombe)
summary (regression1)
summary (regression2)
summary (regression3)
summary (regression4)
```
Here we might want to report these findings. 
> There was a significant, positive relationship between *x* and *y* (F^1,9^=18, p=0.002) in which *x* explained approximately 63% of the variation observed in *y*.

Now you're probably thinking "Great! Let's write up the paper and go home!"

## Visualising Data
Everything we've seen so far has shown us that we have identical relationships between *x* and *y* in the Anscombe dataset. So here's what we should have done first . . . 

We're going to use `ggplot` for this. (We're also going to lean very heavily on [Stella and Ian's great ggplot tutorial](https://ianhandel.github.io/plotting-with-r/index.html) which I recommend if you want to learn more about `ggplot`). We'll walk through building the first chart and then build the others in one go. 

### Understanding ggplot layers

```{r}
ggplot (data=anscombe, aes(x=x1, y=y1))
```

The `ggplot` command first needs to know what *data* to use, (the `data` argument) and then what *aesthetics* ( the `aes` argument). But you will have spotted pretty quickly that there's no data in there yet! We need to start layering information into the `ggplot` command. The two most important types of layers are the geometric elements (`geoms`) and statistical transformations (`stats`). 

We'll begin by adding a layer of points with the `geom_point` function.

```{r}
ggplot (data=anscombe, aes(x=x1, y=y1)) +
  geom_point()
```

Now we're getting somewhere! Let's add a statistical layer. 

```{r}
ggplot (data=anscombe, aes(x=x1, y=y1)) +
  geom_point() +
  stat_smooth (method="lm", se=FALSE)
```

This looks like a proper chart! Still, your research methods teacher will probably shout at you if you don't have the x and y axis start at zero . . . 

```{r}
ggplot (data=anscombe, aes(x=x1, y=y1)) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  expand_limits(x=0, y=0)
```

And she'll also shout at you for not having the x and y axis share sensible scales 

```{r}
ggplot (data=anscombe, aes(x=x1, y=y1)) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  expand_limits(x=0, y=0) +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 10, 2))
```

And why is the background grey? You know that's a waste of ink if you print it out. 

```{r}
ggplot(data=anscombe, aes(x=x1, y=y1)) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  theme_bw() +
  expand_limits(x=0, y=0) +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 10, 2))
```

Maybe you need a title, and better axis labels too?

```{r}
ggplot(data=anscombe, aes(x=x1, y=y1)) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  theme_bw() +
  expand_limits(x=0, y=0) +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 10, 2)) +
  ggtitle("Anscombe Dataset 1") +
  xlab("Variable X1") +
  ylab("Variable Y1")
```

### Building ggplots
Personally, I like to make my ggplots objects like so . . . 
```{r}
Anscombe1 <- ggplot(data=anscombe, aes(x=x1, y=y1)) +
  geom_point() +
  stat_smooth(method="lm", se=FALSE) +
  theme_bw() +
  expand_limits(x=0, y=0) +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  scale_y_continuous(breaks = seq(0, 10, 2)) +
  ggtitle("Anscombe Dataset 1") +
  xlab("Variable X1") +
  ylab("Variable Y1")
```

Then I can call them back much more easily if I want to look at them again

```{r}
Anscombe1
```

And now I can combine these into a grid by taking the first plot `Anscombe1` and adding new column mappings e.g.  `aes(x=x2, y=y2)` and labelling e.g. `ggtitle("Anscombe Dataset 2")`. 
```{r}
Anscombe2 <- Anscombe1 +
  aes(x=x2, y=y2) +
  ggtitle("Anscombe Dataset 2") + xlab("Variable X2") + ylab("Variable Y2")
Anscombe3 <- Anscombe1 +
  aes(x=x3, y=y3) +
  ggtitle("Anscombe Dataset 3") + xlab("Variable X3") + ylab("Variable Y3")
Anscombe4 <- Anscombe1 +
  aes(x=x4, y=y4) +
  ggtitle("Anscombe Dataset 4") + xlab("Variable X4") + ylab("Variable Y4")
#And now let's put them all together!
grid.arrange(Anscombe1, Anscombe2, Anscombe3, Anscombe4, top="This is why you always visualise data first!")
```


Lesson learned, eh?!

## Where Do We Go From Here?
If you spend enough time on it, you can create truly beautiful visualisations in `ggplot2`. Play about with this code and see what you can do with it - share your best attempts! 


I want to create a chart with all the sets overlaid, so I need all the x values in one column, all the y columns in another, and a new factor to tell me which set it belonged to. To change the shape of the data I'm going to use the `tidyr` and `dplyr` packages. (If you're curious, I recommend googling these packages to find some tutorials on how to use them, they're brilliant for data handling) 

```{r}
LongAnscombe <- anscombe %>%
  mutate(observation=seq_len(n()))%>%
  gather(key, value, -observation)%>% 
  separate(key, c("variable", "set") , 1 , convert=TRUE)%>%
  mutate(set=factor(set))%>%
  spread(variable, value)
LongAnscombe
```

Now I want to plot it!

(I quite like [this colour cheatsheet](https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/colorPaletteCheatsheet.pdf) for the `scale_colour_manual` function)

```{r}
AnscombeQuartet <- ggplot (data=LongAnscombe,
                           aes (x=x, y=y, color=set, shape=set)) +
  geom_point() +
  scale_colour_manual(name = "Quartet Set",
                      labels = c("One", "Two", "Three", "Four"),
                      values=c("steelblue4", "springgreen4", "slateblue4", "violetred4")) +
  scale_shape_manual(name = "Quartet Set",
                     labels=c("One", "Two", "Three", "Four"),
                     values=c(16, 17, 18, 15)) +
  stat_smooth(method = "lm", se = FALSE, lwd = 0.5) +
  theme_bw() +
  expand_limits (x=0, y=0) +
  ggtitle ("The Anscombe Quartet") +
  xlab ("The X Variable") +
  ylab ("The Y Variable") +
  facet_grid(. ~ set)
AnscombeQuartet
```

Start messing about with this bit of `ggplot` code to produce your prettiest examples of the Anscombe Quartet. If you feel like you've spent enough time visualising them, here's what I suggest you do next...

`install.packages("datasauRus")`
```{r, echo=FALSE}
library(datasauRus)
```
And then you can do this . . .

```{r}
ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+
  geom_point()+
  theme_void()+
  theme(legend.position = "none")+
  facet_wrap(~dataset, ncol=3)
```

And check out these:
```{r}
stat.desc(datasaurus_dozen_wide, basic=FALSE, norm=FALSE)
```

Got the message yet? ;)

```{r, echo = FALSE}
options(opts)
```




## Intro to ANOVAs {#anovas}

## About This Resource
**Exercise Type:** Theory
*Theory exercises teach the underpinning concepts for statistics or R that you need to know*

**Exercise Suitability:** Beginner

*If you have R Studio installed on your machine and you know what an object is, you can work through this material.*

This is an R translation of a lecture I do explaining the principles behind ANOVAs (or analyses of variances). The teaching material is very much taken from [Grafen & Hails' Modern Statistics for the Life Sciences](http://global.oup.com/uk/orc/biosciences/maths/grafen/) which I love and still reference when I'm trying to think in detail about how statistics work. 

The point of providing this as a full page is to give the R code alongside the materials. **You should feel free to copy and paste the R code directly into your console**. I think this works best if you work alongside the material. Stop to look at your data regularly with `view(data)`, and read the notes within the code chunks about how and why the code has been used. 

## Learning Outcomes

By the end of this sheet you should be able to . . . 

* Describe the principles of how an ANOVA works
* Run an ANOVA in R 
* Interpret the results of an ANOVA


## The R Environment
Here are the packages you'll need for this exercise. (Remember, if you don't have one of these packages you can install it with the command: `install.packages("packagename")` )
```{r}
library(tidyverse)
library(RColorBrewer)
library (knitr)
```



## Analysis of Variance
ANOVAs fall under the GLM formula. People sometimes speak about ANOVAs as if they're a different test,  but Grafen & Hails argue they're a good introduction to the linear model formula because you have to think about variance in frequentist statistics. I'm inclinded to agree. 

### Fake Experiment
We're going to start with a fake experiment to explore this. As you may know, the national animal of Scotland is the unicorn. Scotland has started farming unicorns (n a very welfare friendly manner) to export more magic dust. Unicorns shed magic dust from their horns every morning, and this is gathered by the farmer. 

There's a belief that unicorns shed more dust when they hear music, and so we set up a trial with three farms:

* Farm 1 was our negative control farm, with no intervention.
* Farm 2 was our trial farm, where a radio station tuned to music was played over the speakers. 
* Farm 3 was a positive control farm, and played radio tuned to BBC Radio 4 for thoughtful discussion. 

We recorded the raw weekly dust yield of 10 unicorns on each farm, and the data looks like this:

```{r}
Radio <- tibble (NoRadio = c(150, 130, 121, 90, 98, 100, 98, 100, 113, 111),
                 RadioMusic = c(112, 127, 132, 150, 112, 128, 110, 120, 98, 107),
                 RadioDiscussion = c(75, 98, 88, 83, 83, 77, 75, 84, 93, 99))
# Remember you can view this data in R Studio with `View(Radio)`
```

We could look at the data directly, but let's ask R to make it look a little prettier:

```{r}
kable (Radio,
       col.names = c("No Radio", "Radio Playing Music", "Radio Playing Discussion"), 
       caption = "Raw weekly dust yield (kg) from three unicorn farms")
```

We always want to know **Does x affect y?**. 

In this case, we're asking: **Does radio as a background noise affect the magic dust yield of unicorns?**

### Tidy Data Is Important!
I wanted to write the data in that way because it's quite a logical way to think about it - *for a human*. R prefers data to be [tidy]().

Luckily we can tidy this data extremely easy. 

```{r}
RadioTidy <- Radio %>%
  gather(key = "Radio", value = "DustYield")
# Remember to `View(RadioMelt)` to see what this code has done!
```


### Let's Visualise The Data

Visualising data is always a good stop - but note that in this exercise I'm going to ask you to visualise the data more often than you would in practice. This is because I want to demonstrate different aspects of variance. 

Learning how to do these visualisations is just an extra benefit you get ;)

```{r}
ggplot (RadioTidy, aes (x = Radio, y = DustYield)) + 
     geom_point(aes(shape = Radio, colour = Radio)) +
     labs (title = "Magic Dust Yield(kg) by Unicorn Farm", 
           y = "Dust Yield (Kg)", x = "Radio Condition") +
     scale_y_continuous(limits = c(0, 200)) +
     theme_classic ()
```

In this chart we have plotted the mean yield of every single unicorn by farm. We still want to know whether radio affects yield, but we know that some unicorns are probably high dust yielders, and others might be low yielding unicorns. In other words, we know that individuals vary, and so we want to know what happens to the group *on average*. 

But we can also see here that the groups seem to be different. The amount of variation isn't the same between the groups. In fact, the farm 'Radio Discussion' is more closely clustered together than the others. 


### Partitioning Variation
Let's think a little bit more about what we mean by this question: *does x affect y?*

We want to know what proportion of the variation we observe in y can be explained by x. 


```{r}
# Let's have some new data
examples <- tibble (x.perfect = c(1,2,3,4,5,6,7,8,9,10), 
                    y.perfect = c(1,2,3,4,5,6,7,8,9,10), 
                    x.realistic = c(1,2,3,4,4,6,8,8,9,11), 
                    y.realistic = c(2, 2, 4, 5, 5, 5, 7, 8, 9, 9))
ggplot (data = examples, aes(x.perfect, y.perfect))+
  geom_point () +
  scale_y_continuous(limits =c(0,10)) +
  scale_x_continuous(limits = c(0,10)) +
  labs (title = "100% of the variation in y is explained by x") +
  theme_classic()
```

In a 'perfect' dataset, we see that for every step x increases, y increases by the same. There's a 1:1 relationship between the two. 

Of course, we very rarely ever see this in the real world. 


How does it look with (slightly) more realistic data?

```{r}
ggplot (data = examples, aes(x.realistic, y.realistic))+
  geom_point () +
  scale_y_continuous(limits =c(0,10)) +
  scale_x_continuous(limits = c(0,10)) +
  labs (title = "How much of the variation in y is explained by x?") +
  theme_classic()
```

Most of us looking at this chart would be able to draw a 'line of best fit' describing the relationship between x and y in this example. With a linear model we can calculate this exactly. (This is the exact same equation as you would have learned at school `y = mx + c`, or `predicted y = gradient * (a value of x) + y intercept`)

Let's ask R to fit a linear model for this data:
```{r}
lm(data = examples, y.realistic ~ x.realistic)
```
The `lm` call stands for 'linear model' and is part of R's basic stats code. 

The output here starts with the *Call* which reiterates what we specificed in our `lm` command. Then it gives us:

 * Coefficient of the intercept (where our model line crosses the y axis)
 * Coefficient of x.realistic
 
Since x.realistic is a continuous variable, the x.realistic coeffcient represents the difference in the predicted value of Y for each one-unit difference in x.realistic. 

If that's confusing, let's run this on the perfect data:

```{r}
lm(data = examples, y.perfect ~ x.perfect)
```
The coefficient for x.perfect is *1* - because for every 1 step increase in the value of x, our value of y increases by 1. 


But what about *significance*? Which we all know is the thing you're really looking for here. We can find that out by asking R to summarise the model:


```{r}
summary(lm(data = examples, y.realistic ~ x.realistic))
```
This time we get more information. The coefficients are still listed in the table, alongside their standard error, a T Value and a P value (and look at how significant it is - job done, right?)

At the top we have residuals which we'll come to another day, and at the bottom we have an F statistic and a P Value (these are the ones I'd pay attention to if I were you) and an `Adjusted R-squared`. At the moment this tell us that 92% of the variation in y.realistic can be explained by x.realistic.

And if we wanted, we could add this to our plot from earlier with the addition of a single line of code:

```{r}
ggplot (data = examples, aes(x.realistic, y.realistic))+
  geom_point () +
  scale_y_continuous(limits =c(0,10)) +
  scale_x_continuous(limits = c(0,10)) +
  labs (title = "How much of the variation in y is explained by x?") +
  theme_classic() + # And add a single line below:
  geom_smooth (method = 'lm') 
```

This is easy to do for two continuous variables, but in our unicorn farm example our explanatory variable is categorical. How do we decide how much of a difference in our response (dust yield) can be attributed to the radio condition?


## Calculating Variance

The variability of data is how much the data is scattered around the mean. An ANOVA is simply asking: **Is the mean of each group a better predictor than the mean of all the data?**.  

Let's start by looking at only one farm:

```{r}
Radio %>%
  mutate(UnicornNo = c(1,2,3,4,5,6,7,8,9,10)) %>% 
  # The mutate function adds a new variable just to plot this one specific chart
  # And then we pipe it directly into ggplot, so we're not changing the Radio data
  # Remember you can check this with `View(Radio)`, you'll see 'UnicornNo' doesn't exist.
  ggplot (aes (x = UnicornNo, y = NoRadio)) + 
  geom_point() +
  labs (title = "Magic Dust Yield(kg) for Farm 1", y = "Dust Yield (Kg)") +
  scale_y_continuous(limits = c(0, 200)) +
  scale_x_continuous(breaks = c(0, 2, 4, 6, 8, 10)) + 
  # Edit the above line out to see what happens to the x axis
  theme_classic ()
```

Deviations from the mean will be both positive and negative, and the sum of these deviations will always be zero. First, let's find out the mean dust yield of the farm without any radio ...

```{r}
RadioTidy %>%
  group_by(Radio) %>%
  filter(Radio == "NoRadio") %>%
  summarise(mean = mean(DustYield))
# This looks like a super complicated way to calculate a mean - why do it like this?
# Well, first by piping the data we don't actually save any changes
# We just get to see the result we're interested in. 
# Delete the 'filter' line and see what that looks like. 
```

We could plot this on our chart:
```{r}
Radio %>%
  mutate(count = c(1,2,3,4,5,6,7,8,9,10)) %>%
  ggplot (aes (x = count, y = NoRadio)) + 
  geom_point() +
  labs (title = "Magic Dust Yield(kg) for Farm 1", y = "Dust Yield (Kg)") +
  scale_y_continuous(limits = c(0, 200)) +
  theme_classic () +
  # This new line basically draws a line on our chart based on the mean we just calculated
  geom_hline(yintercept = 111.1) 
```


### Deviation from the group mean
We want to know what the deviation from the mean of the group is for each individual. We can ask R to calculate this for us using the handy `mutate` function:

```{r}
Radio %>%
  mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio)) 
# I'm using Radio here instead of RadioTidy because I'm being a bit lazy
```

And if you don't believe me, you can run those calculations yourself:

```{r}
111.1-150
```
See? Not convinced yet ... ?

```{r}
111.1-130
111.1-121
111.1-90
111.1-98
111.1 - 100
# And so on
```

You can check that the sum of NoRadioDeviation does add to zero (or at least very close to it given some rounding errors):
```{r}
Radio %>%
  mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio)) %>%
  summarise ("Sum of Deviations from No Radio Mean" = sum(NoRadioDeviation))
```

No matter what your dataset looks like, no matter what numbers are in there, this will always be true. 

### Calculating variance from deviations
Because the deviations will always sum to 0, the raw deviations are not very useful. 

So how can we compare the deviation (variance) between two datasets?

If we square the deviations and then sum them we have a more useful measure of variance that we call *Sums of Squares (SS)*

```{r}
Radio %>%
  mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio),
          SquaredNoRadioDeviation = (NoRadioDeviation*NoRadioDeviation))
```

Sums of Squares are useful because they don't sum to 0, but they're still influenced by the number of data points we have (e.g. if we had one more unicorn in the No Radio farm the sum of squares would have to increase). 

So we calculate the variance of the dataset by:


`Variance = Sums of Squares / (n - 1)`


Variance is therefore a measure of the variability of a dataset that takes the size of the dataset into account. And we can use variance to compare variability across different datasets. It's very useful!

R, of course, has a very handy function to calculate variance automatically, `var`:

```{r}
Radio %>%
  summarise("Variance No Radio" = var(NoRadio))
```


But if you're not convinced ...
```{r}
Radio %>%
  mutate (NoRadioDeviation = (mean(NoRadio) - NoRadio),
          SquaredNoRadioDeviation = (NoRadioDeviation*NoRadioDeviation)) %>%
  summarise(SumSquaresNoRadio = sum(SquaredNoRadioDeviation))
```

And then divide that by `n-1`:
```{r}
3006.9/9
```

## Partitioning Variation
Remember, in this experiment we can see that there is variation around dust yields. What we want to know is:
 
 * How much of this variation is due to our explanatory variable (what kind of radio they listened to)
 * *How does x affect y?*
 
 
### Imaginary Scenario 1
Let's imagine a scenario where the condition (our categorical variable) explains almost all of the variance in the dataset. 

It would look like this:
```{r}
Sce1<-tibble (Condition1 = c(99,100,101,99,100,101,99,101,100,101),
              Condition2 = c(120,121,122,120,121,122,120,123,121,120),
              Condition3 = c(83,84,85,85,84,83,83,84,85,86)) %>%
  gather (key = Condition, value = DustYield) %>%
  mutate (Count = c(1:30)) 
# In the code above I've gathered the data into a tidy format 
ImaginaryScenario1 <- Sce1 %>%
  ggplot (aes (x = Count, y = DustYield)) + 
  geom_point(aes(shape = Condition, colour = Condition)) +
  labs (title = "Magic Dust Yield(kg) for Farm 1", y = "Dust Yield (Kg)", x = "Unicorn ID Number") +
  scale_y_continuous(limits = c(0, 200)) +
  theme_classic ()
# I have also made this chart an object because we're going to update it
# It's quicker to do this as an object
# You can compare how we update this chart with how we update the ones above.
ImaginaryScenario1
```


We could fit the mean of the dataset on this:

```{r}
ImaginaryScenario1 +
  geom_hline(yintercept = mean(Sce1$DustYield))
```

Let's compare the overall mean with the group means, and see which one is the best guess for any given data point. Remember - if the condition is explaining a lot of the variation around our response, we'll see less deviation from each group mean than the overall mean. 

```{r}
Sce1 %>%
  group_by(Condition) %>%
  summarise(mean = mean (DustYield))
ImaginaryScenario1 +
  geom_hline(yintercept = mean(Sce1$DustYield)) +
  geom_segment(aes(x =1, y = 100.1, xend =10, yend = 100.1, color = "red")) +
  geom_segment(aes(x = 11, y = 121.0, xend = 20, yend = 121.0, color = "green")) +
  geom_segment (aes(x = 21, y = 84.2, xend = 30, yend = 84.2, color = "blue")) +
  theme (legend.position = "none")
```

In this case, the individual group means are a better description of the group than the overall population mean. Any statistics we were to apply would simply put a number to what we see on this chart. 


So let's look at a second scenario:
```{r}
Sce2<-tibble (Condition1 = c(84,86,123,95,87,110,99,95,121,121),
              Condition2 = c(83,115,85,85, 110,105,84,115,101,100),
              Condition3 = c(84,122,80,80,101,83,83,99, 120,120)) %>%
  gather (key = Condition, value = DustYield) %>%
  mutate (Count = c(1:30)) 
Sce2 %>%
  group_by(Condition) %>%
  summarise(mean = mean (DustYield))
```

Now we chart it, and add our segment lines for each group's mean:

```{r}
Sce2 %>%
  ggplot (aes (x = Count, y = DustYield)) + 
  geom_point(aes(shape = Condition, colour = Condition)) +
  labs (title = "Magic Dust Yield(kg) for Farm 1", y = "Dust Yield (Kg)", x = "Unicorn ID Number") +
  scale_y_continuous(limits = c(0, 200)) +
  theme_classic () +
  geom_hline(yintercept = mean(Sce2$DustYield)) +
  geom_segment(aes(x =1, y = 102.1, xend =10, yend = 102.1, color = "red")) +
  geom_segment(aes(x = 11, y = 98.3, xend = 20, yend = 98.3, color = "green")) +
  geom_segment (aes(x = 21, y = 97.2, xend = 30, yend = 97.2, color = "blue")) +
  theme (legend.position = "none")
```

In this second scenaro, the population mean is not very different from the means of each of the three conditions. In fact, it's hard to see some of those means on the chart! We haven't successfully partitioned off any of the dataset's varaiance by looking at each of the condition means. 

In this second scenario, the explanatory variable is not explaining very much of the response variable at all. 

## Our Scenario
If we plot our unicorn data in the same format:

```{r}
kable (Radio,
       col.names = c("No Radio", "Radio Playing Music", "Radio Playing Discussion"), 
       caption = "Raw weekly dust yield (kg) from three unicorn farms")
RadioTidy %>%
  mutate(count = c(1:30)) %>%
  ggplot (aes (x = count, y = DustYield)) + 
  geom_point(aes(shape = Radio, colour = Radio)) +
  labs (title = "Magic Dust Yield(kg)", y = "Dust Yield (Kg)", x = "Unicorn ID Number") +
  scale_y_continuous(limits = c(0, 200)) +
  theme_classic ()
```

### A word on significance
We're beginning to get closer to the real definition of significance. 

**Significance:** Is the variability explained by our model greater than we would expect by chance alone. 

And this is what variance can answer. 

### Going Back to Sums of Squares
In our two imaginary scenarios above we were (roughly) partitioning variances by drawing lines on our charts - much like we could draw a line of best fit on the perfect data chart earlier. 

When we run a model, we want to describe that chart mathematically. 

When we calculated variance earlier, we calculated the variance in the condition mean mean (e.g. the variance for No Radio data). We can call this the **Error Sum of Squares (SSE)** which is the sum of the squares of the deviations of the data around the means of each group. 

We could also calculate ...

* **Total Sums of Squares (SSY)**, the sums of squares of the deviations of the data around the population mean. 
* **Condition Sums of Squares (SSF)**, the sums of squares of the deviations of each condition mean from the population mean. 

But why would we use any sums of squares when we already establishd that variance was a better measure?


Let's rearrange our data so we have for each 'row' the condition mean and total mean. We're going to call this dataset `MFY` for short because:

* M is the global mean
* F is the condition mean
* Y is the 'response variable'

```{r}
MFY <- RadioTidy %>%
  mutate ("Y" = DustYield) %>%
  mutate ("M" = mean(Y)) %>%
  # If we now ask R to group the data, it will calculate the mean per group:
  group_by(Radio) %>%
  mutate ("F" = mean(Y)) %>%
  # Remember to ungroup after!
  ungroup() 
#  I suggest you `view(MFY)`
```

We can also calculate:

* MY (The data - Dataset mean)
* MF (The condition mean - dataset mean)
* FY (The data - condition mean)

```{r}
MFY <- MFY %>%
 mutate (MY = (Y-M),
         MF = (F-M),
         FY = (Y - F))
```

We've calculated a lot of deviations here - but we said we always wanted to square those deviations, so:

```{r}
MFY <- MFY %>%
  mutate (MY2 = (MY*MY),
          MF2 = (MF*MF),
          FY2 = (FY*FY))
```

MY2, MF2 & FY2 are the squares of the deviations, but we need a sum of squares for each one. 

```{r}
MFY %>%
  summarise(SumSquareMY = sum(MY2),
            SumSquareMF = sum(MF2),
            SumSquareFY = sum(FY2))
```

But we also have one more bit of information we need to consider - what we call **degrees of freedom**. Let's say we want to put those sum squares into a nice table. We could also add in a row talking about the degrees of freedom. 

There are degrees of freedom for:

* **M** - which is 1. There is only 1 bit of information in the whole group mean
* **F** - which is 3. There are only 3 means in the conditions. 
* **Y** - which is 30. We have 30 data points to observe. 

If you were to calculate the degrees of freedom (dof) for MY, MF and FY - you can run the same calculation:

* **MY** (Ydof - Mdof = 30 - 1 = 29)
* **MF** (Fdof - Mdof = 3 - 1 = 2)
* **FY** (Ydof - Fdof = 30 - 3 = 27)


We can calculate something called **mean squares** which is:
`Mean Squares = Sum of Squares / Degrees of Freedom`

So:

* **MS-MY** = (12053.2/ 29) = 415.6276
* **MS-MF**  = (6301.4/2) =  3150.7
* **MS-FY**  = (5751.8/27) =  213.0296

And this is pretty much all we can do with the data. 

```{r}
kable(MFY %>%
        summarise(SumSquareMY = sum(MY2),
                  SumSquareMF = sum(MF2),
                  SumSquareFY = sum(FY2),
                  MeanSquareMY = sum(MY2)/29,
                  MeanSquareMF = sum(MF2)/2,
                  MeanSquareFY = sum(FY2)/27))
```

Earlier we said variance was important because it took into account the size of the dataset. 

Mean Squares are very similar to the variance calculation, because they're a measure of deviation in relation to the size of the dataset. 

## The Importance of Mean Squares
If the condition (radio) has no effect on the data, then the variation we would see between the farms would be similar to the variation we saw within any given farm. It would be like Scenario 2, where the mean of the farm was no more use to us than the mean of the overall population. 

If that were the case: the `Condition Mean Square (FMS)  / Error Mean Square (EMS) = 1`

Let's look again at that last table.
```{r}
kable(MFY %>%
        summarise(SumSquareMY = sum(MY2),
                  SumSquareMF = sum(MF2),
                  SumSquareFY = sum(FY2),
                  MeanSquareMY = sum(MY2)/29,
                  MeanSquareMF = sum(MF2)/2,
                  MeanSquareFY = sum(FY2)/27))
```

And now we will take the Condition Mean Square and Error Mean Squares from that table:

* FMS = 3150.7
* ENS = 213.0296

And we'll call this the F Ratio. 

```{r}
3150.7/213.0296
```

## F Ratios and F Distributions
Because the mean squares are standardised by the size of the dataset, we can mathematically calculate the range and likelihood of any F-Ratio. This is called the F Distribution. 

You can [look up](http://www.socr.ucla.edu/Applets.dir/F_Table.html) F distributions for an alpha level of 0.05. [Click here to go directly there](http://www.socr.ucla.edu/Applets.dir/F_Table.html#FTable0.05). We are interested in the ratio between 2 and 27 degrees of freedom, which gives us a critical F ratio of **3.3541**, which our F Ratio (14.79) is much bigger than. 

In fact, if we were to plot a curve of the F Distribution between 2 and 27 degrees of freedom, less than 0.001% of the total area would be more than 14.79.

## In Conclusion . . .
> The probability of getting an F-Ratio as large as 14.8 (or larger), if the null hypothes (x has no effect on y) is true, is less than 0.001
> Or: F (between 2 and 27 degrees of freedom) = 14.79, and P < 0.001
> **CONGRATULATIONS!**
> You just ran an ANOVA completely by hand. 
# Prove It
Of course, we don't run ANOVAs by hand. We don't use all these steps to run an ANOVA. Instead we go back to the RadioTidy data, which if you `View(RadioTidy)`, you will note doesn't have any of our squares or sum squares or mean squares calculated.

And we ask R to run an ANOVA:

```{r}
ANOVA <- aov(DustYield ~ Radio, data = RadioTidy)
summary(ANOVA)
```

And that command `summary(ANOVA)` gives us a table exactly like the one we calculated by ourselves.

When we say that radio has a significant effect on the dust yield of unicorns in an ANOVA (F^2,27^=14.79, P<0.001) - you now know exactly what those numbers refer to. 

And now that you've run it by yourself, what would you recommend to the unicorn farmers of Scotland?





## Wide and Tall data {#widetalldata}


### About This Exercise
**Exercise Type:** Guidance

*Guidance documents explore how to practice R*

**Exercise Suitability:** Intermediate

*If you are getting comfortable running your own code, this exericse will begin to push you forward*


These are some notes on changing the format of data from tall to wide and vice versa. Like everything R there are many ways to do this and I used to be a big fan of the `reshape2` package until one day I had a Eureka moment about how `gather` and `spread` work.

I'm writing this out mainly because I found the online tutorials thought this was obvious and it wasn't to me!

### Learning Outcomes
By the end of this workshop you should be able to . . .

* Reshape data at will
 + (Optional: Make preserve-related puns about spreadable data)


## The R Environment
Here are the packages you'll need for this exercise. (Remember, if you don't have one of these packages you can install it with the command: `install.packages("packagename")`

```{r}
library(tidyverse)
```



## The Observation
As [everyone online says](http://vita.had.co.nz/papers/tidy-data.html) - tidy data has **one observation per row**. My headaches came from dealing with the publicly available [National Student Survey](https://www.officeforstudents.org.uk/advice-and-guidance/student-information-and-data/national-student-survey-nss/) data - the format of which *seems* tidy to the uninitiated. The data is presented with each subject at each university as a row - that's an observation, surely?


### The Data
```{r}
data <- tibble (group1 = c("A", "A", "A", "A", "B", "B", "B", "B"),
                group2 = c("X", "X", "Y", "Y", "X", "X", "Z", "Z"),
                q = c ("Q1", "Q2","Q1", "Q2", "Q1", "Q2", "Q1", "Q2"),
                disagree = c(0.8, 0.3, 0.8, 0.2, 0.7, 0.5, 0.6, 0.3),
                neutral = c(0.05, 0.4, 0.1, 0.3, 0.1, 0.4, 0.2, 0.3),
                agree = c(0.15, 0.3, 0.1, 0.5, 0.2, 0.1, 0.2, 0.4),
                n = c(121, 121, 140, 140, 50, 50, 57,57)) 
data
```

However, for a lot of what I was trying to do, I needed to know how many students responded at each level - how many agreed with Q1, Q2, etc, in each nested group. I wanted my data taller. 

(I'm using this specific format not because it's a nice format, but because it's a format that exists in the world and that lots of people make big decisions on.)

## Gather
The `gather` command from `tidyverse` is a quick way to smush this data into a tall format. The `gather` command creates two new columns, the `key` column which collects your old column names and your `value` column which collects the row values (fairly self-explanatory).

The trick with `gather` is that it will smush everything it can into those two columns, so you need to tell it which columns *not* to include. This is the step that eluded me for a whole afternoon once, so I'm stating it very obviously here.

```{r}
talldata <- data %>%
  gather (key = LikertScale, value = PercRespondents,
          -group1,
          -group2,
          -q,
          -n)
talldata
```

### NB - Think About Your Variable Names
I once spent a whole afternoon unable to recreate an error message I was getting with `gather` - only to realise long after home time that I had sensibly called the `key` variable *question* which was a variable name that already existed in my dataset. R was re-writing the variable every time it gathered the data. 

The take home? **Make sure your key and value names are unique!**

## Spread
What if, after all that, you realise that you never wanted your data gathered at all? `spread` is here to rescue you. 

Just as before, `spread` wants to know the `key` and the `value`, but this time, it will split those two columns into multiple columns. This time we want all that data to be spread out like marmalade on toast, so we don't exclude any columns (in fact, try excluding the columns and see what spread says. )

```{r}
widedata <- talldata %>%
  spread (key = LikertScale, value = PercRespondents)
widedata
```


## Final Thoughts
There is far more that `tidyverse` can do to reshape your data, which I might write up at some point, but these are two great commands that eluded me for a while. Hopefully this is a very quick and simple explanation for you to look up. :)
